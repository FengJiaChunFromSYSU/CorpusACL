Parse Forest Computation of Expected Governors
Helmut Schmid
Institute for Computational Linguistics
University of Stuttgart
Azenbergstr. 12
70174 Stuttgart, Germany
schmid@ims.uni-stuttgart.de
Mats Rooth
Department of Linguistics
Cornell University
Morrill Hall
Ithaca, NY 14853, USA
mats@cs.cornell.edu
Abstract
In a headed tree, each terminal word
can be uniquely labeled with a governing word and grammatical relation.
This labeling is a summary of a syntactic analysis which eliminates detail,
reflects aspects of semantics, and for
some grammatical relations (such as
subject of finite verb) is nearly uncontroversial. We define a notion
of expected governor markup, which
sums vectors indexed by governors and
scaled by probabilistic tree weights.
The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector
sums and scaling by inside probability
and flow.
1 Introduction
A labeled headed tree is one in which each nonterminal vertex has a distinguished head child,
and in the usual way non-terminal nodes are labeled with non-terminal symbols (syntactic categories such as NP) and terminal vertices are
labeled with terminal symbols (words such as
The governor algorithm was designed and implemented
in the Reading Comprehension research group in the 2000
Workshop on Language Engineering at Johns Hopkins University. Thanks to Marc Light, Ellen Riloff, Pranav Anand,
Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen
for discussion and assistance. Oral presentations were made
at that workshop in August 2000, and at the University of
Sussex in January 2001. Thanks to Fred Jelinek, John Carroll, and other members of the audiences for their comments.
S ������
NP� ���� 
Peter
VP ������
V �����
reads
NP �  �� 
NP �  � 
D���� 
every
N �  �� 
paper
PP:on �� � 
P:on�!
on
NP �� � 
N �� � 
markup
Figure 1: A tree with percolated lexical heads.
reads).1 We work with syntactic trees in which
terminals are in addition labeled with uninflected
word forms (lemmas) derived from the lexicon.
By percolating lemmas up the chains of heads,
each node in a headed tree may be labeled with
a lexical head. Figure 1 is an example, where lexical heads are written as subscripts. We use the
notation
"$#&%('
for the lexical head of a vertex
%
,
and )
#&%('
for the ordinary category or word label
of
%
.
The governor label for a terminal vertex
%
in
such a labeled tree is a triple which represents
the syntactic and lexical environment at the top
of the chain of vertices headed by
%
. Where 0 is
the maximal vertex of which
%
is a head vertex,
and 021 is the parent of 0 , the governor label for
%
1
Headed trees may be constructed as tree domains, which
are sets of addresses of vertices. 0 is used as the relative address of the head vertex, negative integers are used as relative
addresses of child vertices before the head, and positive integers are used as relative addresses of child vertices after
the head. A headed tree domain is a set of finite sequences
of integers 3 such that (i) if 4656783 , then 49783 ; (ii) if 46@A783 and
BDCFEHG
@ or @
GFEHCFB
, then 4
E
783 .
position word governor label
1 Peter I NP,S,readP
2 reads I S,STARTC,startwP
3 every I D,NP,paperP
4 paper I NP,VP,readP
5 on I P:ON,PP:ON,markupP
6 markup I NP,PP:ON,paperP
Figure 2: Governor labels for the terminals in the
tree of Figure 1. For the head of the sentence,
special symbols startc and startw are used as the
parent category and parent lexical governor.
is the tuple I8)
#
0
'Q
)
#
021
'Q�"$#
0R1
'
P .2 Governor labels
for the example tree are given in Figure 2.
As observed in Chomsky (1965), grammatical
relations such as subject and object may be reconstructed as ordered pairs of category labels,
such as I NP,SP for subject. So, a governor label
encodes a grammatical relation and a governing
lexical head.
Given a unique tree structure for a sentence,
governor markup may be read off the tree. However, in view of the fact that robust broad coverage
parsers frequently deliver thousands, millions, or
thousands of millions of analyses for sentences of
free text, basing annotation on a unique tree (such
as the most probable tree analysis generated by a
probabilistic grammar) appears arbitrary.
Note that different trees may produce the same
governor labels for a given terminal position.
Suppose for instance that the yield of the tree in
Figure 1 has a different tree analysis in which the
PP is a child of the VP, rather than NP. In this
case, just as in the original tree, the label for the
fourth terminal position (with word label paper)
is I NP,VP,readP . Supposing that there are only
two tree analyses, this label can be assigned to the
fourth word with certainty, in the face of syntactic ambiguity. The algorithm we will define pools
governor labels in this way.
2 Expected Governors
Suppose that a probabilistic grammar licenses
headed tree analyses SUT
QWVXVXVXQ
S ! for a sentence Y ,
and assigns them probabilistic weights `aT
QWVXVXVXQ
` ! .
2
In a headed tree domain, b is a head of c if b is of the
form c
Bed
for @gfih .
that NP SC deprive .95 .99
would MD2 VFP deprive .98 1
deprive S STARTC startw 1 1
all DETPL2 NC student .83 .98
beginning NSG1 NPL1 student .75 .98
students NP VFP deprive .82 .98
NP VGP begin .16
of PP NP student .53
PP VFP deprive .38 .99
their DETPL2 NC lunch .92 .98
high ADJMOD NPL2 lunch .78 .23
ADJMOD NSG2 school .15 .76
school NCHAIN NPL1 lunch .16
NSG1 NPL1 lunch .76 .98
lunches NP PP of .91 .98
. PERC S deprive .88 .86
PERC X deprive .14
Figure 3: Expected governors in the sentence That would
deprive all beginning students of their high school lunches.
For a label p in column 2, column 3 gives qsrXput as computed with a PCFG weighting of trees, and column 4 gives
qsrXput as computed with a head-lexicalized weighting of
trees. Values below 0.1 are omitted. According to the lexicalized model, the PP headed by of probably attaches to VFP
(finite verb phrase) rather than NP.
Let vwT
QWVXVXVXQ
v ! be the governor labels for word
position x determined by SUT
QWVXVXVXQ
S ! respectively.
We define a scheme which divides a count of 1
among the different governor labels.
For a given governor tuple v , let
y #
v
' def
  `R
 T�& ! ` 
V
(1)
The definition sums the probabilistic weights of
trees with markup v , and normalizes by the sum
of the probabilities of all tree analyses of Y .
The definition may be justified as follows. We
work with a markup space 

,
where

is the set of category labels and

is the
set of lemma labels. For a given markup triple v ,
let 
ed  fhg i
be the function which maps v to 1, and vj1 to 0
for vk1ml

v . We define a random variate
n
d Trees fpo  fhg irq
which maps a tree S to

 , where v is the governor markup for word position x which is determined by tree S . The random variate n
is defined on labeled trees licensed by the probabilistic
grammar. Note that o  fg irq is a vector space
(with pointwise sums and scalar products), so that
expectations and conditional expectations may be
defined. In these terms,
y
is the conditional expectation of n
, conditioned on the yield being Y .
This definition, instead of a single governor label for a given word position, gives us a set of
pairs of a markup v and a real number
y #
v
'
in
[0,1], such that the real numbers in the pairs sum
to 1. In our implementation (which is based on
Schmid (2000a)), we use a cutoff of 0.1, and print
only indices v where
y #
v
'
is above the cutoff.
Figure 3 is an example.
A direct implementation of the above definition
using an iteration over trees to compute
y
would
be unusable because in the robust grammar of English we work with, the number of tree analyses
for a sentence is frequently large, greater than sWt(u
for about 1/10 of the sentences in the British National Corpus. We instead calculate
y
in a parse
forest representation of a set of tree analyses.
3 Parse Forests
A parse forest (see also Billot and Lang (1989))
in labeled grammar notation is a tuple v

I8wx�
Qy
�
Q
iz�
Q{
�
Q}|
�~P where
I8wg�
Qy
�
Q
iz�
Q{
�~P is a context free grammar (consisting of non-terminals w� , terminals
y
� , rules i � , and a start symbol
{
� ) and
|
� is a function which maps elements of w�
to non-terminals in an underlying grammar
 
I8w
QyrQ
i
Q{
P and elements of
y
� to terminals in

. By using
|
� on symbols on the left
hand and right hand sides of a parse forest rule,
|
� can be extended to map the set of parse forest
rules iu� to the set of underlying grammar rules
i .
|
� is also extended to map trees licensed
by the parse forest grammar to trees licensed by
the underlying grammar. An example is given in
figure 4.
Where }w�H
y
�Hiz� , let g�
#

'
be the set of
trees licensed by I8w�
Qy
�
Q
iz�
Q{
�P which have
root symbol  in the case of a symbol, and the set
of trees which have  as the rule expanding the
root in the case or a rule. g
#

'
is defined to be
the multiset image of g �
#

'
under
|
� . g
#

'
is
the multiset of inside trees represented by parse
STf NPT VPT
VPT}f VT NP
VPT}f VP PPT
NP f NP PPT
NP f DT NT
PPTf PT NP
VPUf VT NP
NPf N
NPTf Peter
VTf reads
DT f every
NTf paper
PTf on
Nf markup
Figure 4: Rule set i� of a labeled grammar representing two tree analyses of John reads every
paper on markup. The labeling function drops
subscripts, so that
|
�
#
VPT
' 
VP.
forest symbol or rule  .3 Let

�
#

'
be the set of
trees in g�
#{
�
'
which contain  as a symbol or
use  as a rule.
 #

'
is defined to be the multiset
image of

�
#

'
under
|
� .
 #

'
is the multiset
of complete trees represented by the parse forest
symbol or rule  .
Where ` is a probability function on trees licensed by the underlying grammar and  is a symbol or rule in v ,
 #

' def
 
�8U
`
#
S
'
(2)
a#

' def
  �$ `
#
S
'
 �8$X 2 `
#
S
'
V
(3)
 #

'
is called the inside probability for  and
a#

'
is called the flow for  .4
Parse forests are often constructed so that all
inside trees represented by a parse forest nonterminal ���wx� have the same span, as well as the
same parent category. To deal with headedness
and lexicalization of a probabilistic grammar, we
construct parse forests so that, in addition, all inside trees represented by a parse forest nonterminal have the same lexical head. We add to the labeled grammar a function �F� which labels parse
forest symbols with lexical heads. In our implementation, an ordinary context free parse forest is
3
We use multisets rather than set images to achieve correctness of the inside algorithm in cases where � represents
some tree more than once, something which is possible given
the definition of labeled grammars. A correct parser produces a parse forest which represents every parse for the input sentence exactly once.
4
These quantities can be given probabilistic interpretations and/or definitions, for instance with reference to conditionally expected rule frequencies for flow.
PF-INSIDE(v
Q��
)
1 Initial. float array

o i � �w � 
y
� q� 0
2 for
%F�jy
�
3 do

o
%
q���s
4 for n
in iz� in bottom-up order
5 do

o
n
q��
�x#8|
�
# n '�'9�
  ���} }

o
%
q
6

o lhs
# n '
q��

o lhs
# n '
q(�

o
n
q
7 return

Figure 5: Inside algorithm.
first constructed by tabular parsing, and then in a
second pass parse forest symbols are split according to headedness. Such an algorithm is shown
in appendix B. This procedure gives worst case
time and space complexity which is proportional
to the fifth power of the length of the sentence.
See Eisner and Satta (1999) for discussion and an
algorithm with time and space requirements proportional to the fourth power of the length of the
input sentence in the worst case. In practical experience with broad-coverage context free grammars of several languages, we have not observed
super-cubic average time or space requirements
for our implementation. We believe this is because, for our grammars and corpora, there is limited ambiguity in the position of the head within
a given category-span combination.
The governor algorithm stated in the next section refers to headedness in parse forest rules.
This can be represented by constructing parse forest rules (as well as ordinary grammar rules) with
headed tree domains of depth one.5 Where 0 is
a parse forest symbol on the right hand side of a
parse forest rule n
, we will simply state the condition "0 is the head of n
".
The flow and governor algorithms stated below call an algorithm PF-INSIDE
#
v
Q��z'
which
computes inside probabilities in v , where
�
is a
function giving probability parameters for the underlying grammar. Any probability weighting of
trees may be used which allows inside probabilities to be computed in parse forests. The inside
5
See footnote 1. Constructed in this way, the first rule in
parse forest in Figure 4 has domain ��7��he�
BW�
, and labeling
function �W�7� S�����8�he� NP������
B
� VP��
�
. When parse forest
rules are mapped to underlying grammar rules, the domain is
preserved, so that �  applied to the parse forest rule just described is the tree with domain �}7����he�
BW�
and label function
�W�7�� S����8�h� NP����
B
� VP�
�
. 7 is the empty string.
PF-FLOW(v
Q 
)
1 Initial. float array

o i � �w � 
y
� q� 0
2

o
{
�$q���s
3 for n
in iz� in top-down order
4 do

o
n
q��
8�  ��
8� � ��} }X�

o lhs
# n '
q
5 for
%
in rhs
# n '
6 do

o
%
q��

o
%
q(�

o
n
q
7 return

Figure 6: Flow algorithm.
algorithm for ordinary PCFGs is given in figure
5. The parameter
�
maps the set of underlying
grammar rules i which is the image of
|
� on
|D�
to reals, with the interpretation of rule probabilities. In step 5,
|
� maps the parse forest rule
n
to a grammar rule
|
�
# n '
which is the argument
of
�
. The functions lhs and rhs map rules to their
left hand and right hand sides, respectively.
Given an inside algorithm, the flow

may be
computed by the flow algorithm in Figure 6, or
by the inside-outside algorithm.
4 Governors Algorithm
The governor algorithm annotates parse forest
symbols and rules with functions from governor
labels to real numbers. Let S be a tree in the parse
forest grammar, let
%
be a symbol in S , let 0 be the
maximal symbol in S of which
%
is a head, or
%
itself if
%
is a non-head child of its parent in S , and
let 0R1 be the parent of 0 in S . Recall that
�
 8�
�
�> �$�X (4)
is a vector mapping the markup triple
I
|
�
#
0
'Q}|
�
#
0 1
'Q
��
#
0 1
'
P to 1 and other markups
to 0. We have constructed parse forests such
that I
|
�
#
0
'Q}|
�
#
0R1
'Q
� �
#
021
'
P agrees with the
governor label for the lexical head of the node
corresponding to
%
in
|
�
#
S
'
.
A parse forest tree S and symbol
%
in S thus determine the vector (4), where 0 and 0 1 are defined
as above. Call the vector determined in this way

#
S
Q�%('
. Where
%
is parse forest symbol in v and
n
is a parse forest rule in v , let
� #&%(' def
  �89� `
#8|
�
#
S
'�'

#
S
Q�%9'
 �9��� `
#8|
�
#
S
'�' (5)
PF-GOVERNORS(v
Q��
)
1

� PF-INSIDE
#
v
Q��u'
2

� PF-FLOW
#
v
Q  '
3 Initialize array �
o i��zwx�
y
�$q to empty maps from governor labels to float
4 �
o
{
�$q��

  �  8� startc� startw
5 for n
in iz� in top-down order
6 do �
o
n
q�
8�  ��
8� � ��} }X�
�
o lhs
# n '
q
7 for 0 in rhs
# n '
8 do if 0 is the head of n
9 then �
o 0Rq��
�
o 0q(�
�
o
n
q
10 else �
o 0q��
�
o 0Rq(�

o
n
q

9� 8� 9� � � �� �8� �m� � ��} }�
11 return �
Figure 7: Parse forest computation of governor vector.
� # n ' def
  �9 � `
#8|
�
#
S
'�'

#
S
Q
lhs
# n '�'
 �9��� `
#8|
�
#
S
'�'
V
(6)
Assuming that v

I8w �
Qy
�
Q
i �
Q{
�
Q}|
� P is
a parse forest representing each tree analysis for
a sentence exactly once, the quantity
y
for terminal position x (as defined in section 1) is found
by summing � #&%9'
for terminal symbols
%
in
y
�
which have string position x .6
The algorithm PF-GOVERNORS is stated in Figure 3. Working top down, if fills in an array
�
o� q which is supposed to agree with the quantity � #
�
'
defined above. Scaled governor vectors
are created for non-head children in step 10, and
summed down the chain of heads in step 9. In
step 6, vectors are divided in proportion to inside
probabilities (just as in the flow algorithm), because the set of complete trees for the left hand
side of n
are partitioned among the parse forest
rules which expand the left hand side of n
.
Consider a parse forest rule n
, and a parse forest symbol 0 on its right hand side which is not
the head of n
. In each tree in

�
# n '
, 0 is the top
of a chain of heads, because 0 is a non-head child
in rule n
. In step 10, the governor tuple describing
the syntactic environment of 0 in trees in

�
# n '
(or rather, their images under
|
� ) is constructed
6
This procedure requires that symbols in �  correspond
to a unique string position, something which is not enforced
by our definition of parse forests. Indeed, such cases may
arise if parse forest symbols are constructed as pairs of grammar symbols and strings (Tendeau, 1998) rather than pairs
of grammar symbols and spans. Our parser constructs parse
forests organized according to span.
as

   �    � ��} }ž �   � � �� � . The scalar multiplier

o
n
q is
 �89� } `
#
S
'
 �89��� `
#
S
'
Q
the relative weight of trees in

�
# n '
. This is appropriate because � #
0
'
as defined in equation (5)
is to be scaled by the relative weight of trees in

�
#
0
'
.
In line 9 of the algorithm, �
is summed into the
head child 0 . There is no scaling, because every
tree in

�
# n '
is a tree in

�
#
0
'
.
A probability parameter vector
�
is used in the
inside algorithm. In our implementation, we can
use either a probabilistic context free grammar, or
a lexicalized context free grammar which conditions rules on parent category and parent lexical
head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll
and Rooth, 1998). The requisite information is directly represented in our parse forests by

� and
�� . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation
of PCFG inside probabilities, or head-lexicalized
inside probabilities. However, in both cases the
algorithm requires that the parse forest symbols
be split according to heads, because of the reference to � � in line 10. Construction of headmarked parse forests is presented in the appendix.
The LoPar parser (Schmid, 2000a) on which
our implementation of the governor algorithm is
based represents the parse forest as a graph with
at most binary branching structure. Nodes with
more than two daughter nodes in a conventional
parse forest are replaced with a right-branching
tree structure and common sub-trees are shared
between different analyses. The worst-case space
complexity of this representation is cubic (cmp.
Billot and Lang (1989)).
LoPar already provided functions for the computation of the head-marked parse forest, for the
flow computation and for traversing the parse forest in depth-first and topologically-sorted order
(see Cormen et al. (1994)). So it was only necessary to add functions for data initialization, for the
computation of the governor vector at each node
and for printing the result.
5 Pooling of grammatical relations
The governor labels defined above are derived
from the specific symbols of a context free grammar. In contrast, according to the general markup
methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism. The same markup labels
should be produced by different systems, making
it possible to substitute one system for another,
and to compare systems using objective tests.
Carroll et al. (1998) and Carroll et al. (1999)
propose a system of grammatical relation markup
to which we would like to assimilate our proposal.
As grammatical relation symbols, they use atomic
labels such as dobj (direct object) an ncsubj (nonclausal subject). The labels are arranged in a hierarchy, with for instance subj having subtypes ncsubj, xsubj, and csubj.
There is another problem with the labels we
have used so far. Our grammar codes a variety
of features, such as the feature VFORM on verb
projections. As a result, instead of a single object
grammatical relation I NP,VPP , we have grammatical relations I NP,VP.NP , I NP,VP.FINP , I NP,VP.TOP ,
I NP,VP.BASEP , and so forth. This may result in
frequency mass being split among different but
similar labels. For instance, a verb phrase will
have read every paper might have some analyses in which read is the head of a base form
VP and paper is the head of the object of read,
and others where read is a head of a finite form
VP, and paper is the head of the object of read.
In this case, frequencies would be split between
I NP,VP.BASE,readP and I NP,VP.FIN,readP as governor labels for paper.
To address these problems, we employ a pooling function

i which maps pairs of categories
to symbols such as ncsubj or obj. The governor tuple I8)
#
0
'Q
)
#
021
'Q�"$#
0R1
'
P is then replaced by
I

i
#
)
#
0
'Q
)
#
0 1
'�'Q�"�#
0
'
P in the definition of the
governor label for a terminal vertex
%
. Line 10
of PF-GOVERNORS is changed to
�
o 0q��
�
o 0q&�

o
n
q
U�
�
X9� 8� 9� � ��� }�ž �$� � � �� �
V
More flexibility could be gained by using a rule
and the address of a constituent on the right hand
side as arguments of

i . This would allow the
following assignments.

i
#
VP.FIN f VC.FIN' NP NP
Q
s
' 
dobj

i
#
VP.FIN f VC.FIN' NP NP
Q���' 
obj2

i
#
VP.FIN f VC.FIN' VP.TO
Q
s
' 
xcomp

i
#
VP.FIN f VP.FIN' VP.TO
Q
s
' 
xmod
The head of a rule is marked with a prime. In the
first pair, the objects in double object construction
are distinguished using the address. In each case,
the child-parent category pair is I NP,VP.FINP , so
that the original proposal could not distinguish the
grammatical relations. In the second pair, a VP.TO
argument is distinguished from a VP.TO modifier
using the category of the head. In each case, the
child-parent category pair is I VP.TO,VP.FINP . Notice that in Line 10 of PF-GOVERNORS, the rule
n
is available, so that the arguments of

i could
be changed in this way.
6 Discussion
The governor algorithm was designed as a component of Spot, a free-text question answering
system. Current systems usually extract a set
of candidate answers (e.g. sentences), score
them and return the n highest-scoring candidates
as possible answers. The system described in
Harabagiu et al. (2000) scores possible answers
based on the overlap in the semantic representations of the question and the answer candidates. Their semantic representation is basically
identical to the head-head relations computed by
the governor algorithm. However, Harabagiu
et al. extract this information only from maximal probability parses whereas the governor algorithm considers all analyses of a sentence and
returns all possible relations weighted with estimated frequencies. Our application in Spot works
as follows: the question is parsed with a specialized question grammar, and features including
the governor of the trace are extracted from the
question. Governors are among the features used
for ranking sentences, and answer terms within
sentences. In collaboration with Pranav Anand
and Eric Breck, we have incorporated governor
markup in the question answering prototype, but
not debugged or evaluated it.
Expected governor markup summarizes syntactic structure in a weighted parse forest which
is the product of exhaustive parsing and insideoutside computation. This is a strategy of
dumbing down the product of computationally intensive statistical parsing into unstructured
markup. Estimated frequency computations in
parse forests have previously been applied to
tagging and chunking (Schulte im Walde and
Schmid, 2000). Governor markup differs in that
it is reflective of higher-level syntax. The strategy has the advantage, in our view, that it allows
one to base markup algorithms on relatively sophisticated grammars, and to take advantage of
the lexically sensitive probabilistic weighting of
trees which is provided by a lexicalized probability model.
Localizing markup on the governed word increases pooling of frequencies, because the span
of the phrase headed by the governed item is
ignored. This idea could be exploited in other
markup tasks. In a chunking task, categories and
heads of chunks could be identified, rather than
categories and boundaries.
References
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the ACL, University of British Columbia, Vancouver, B.C., Canada.
Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings
of Third Conference on Empirical Methods in Natural Language Processing, Granada, Spain.
John Carroll, Antonio Sanfilippo, and Ted Briscoe.
1998. Parser evaluation: a survey and a new proposal. In Proceedings of the International Conference of Language Resources and Evaluation, pages
447�454, Granada, Spain.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceedings of the EACL99 workshop on Linguistically Interpreted Corpora (LINC), Bergen, Norway,
June.
Eugene Charniak. 1993. Statistical Language Learning. The MIT Press, Cambridge, Massachusetts.
Eugene Charniak. 1995. Parsing with contextfree grammars and word statistics. Technical Report CS-95-28, Department of Computer Science,
Brown University.
Noam Chomsky. 1965. Aspects of the Theory of Syntax. M.I.T. Press, Cambridge, MA.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1994. Introduction to Algorithms. The MIT Press, Cambridge, Massachusetts.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics (ACL'99), College Park, MD.
Jason Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the 4th
international Workshop on Parsing Technologies,
Cambridge, MA.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. G�rju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge
for answer engines. In Proceedings of the Ninth
Text REtrieval Conference (TREC 9), Gaithersburg,
MD, USA, November.
Helmut Schmid. 2000a. LoPar: Design and Implementation. Number 149 in Arbeitspapiere des Sonderforschungsbereiches 340. Institute for Computational Linguistics, University of Stuttgart.
Helmut Schmid. 2000b. Lopar man pages. Institute for Computational Linguistics, University of
Stuttgart.
Sabine Schulte im Walde and Helmut Schmid. 2000.
Robust german noun chunking with a probabilistic
context-free grammar. In Proceedings of the 18th
International Conference on Computational Linguistics, pages 726�732, Saarbr�cken, Germany,
August.
Frederic Tendeau. 1998. Computing abstract decorations of parse forests using dynamic programming
and algebraic power series. Theoretical Computer
Science, (199):145�166.
A Relation Between Flow and
Inside-Outside Algorithm
The inside-outside algorithm computes inside
probabilities

o
%
q and outside probabilities �9o
%
q .
We will show that these quantities are related
to the flow
a#&%('
by the equation

o
%
q

�9o
%
q

o
%
q&�

o
{
�aq .

o
{
�$q is the inside probability of
the root symbol, which is also the sum of the
probabilities of all parse trees.
According to Charniak (1993), the outside
probabilities in a parse forest are computed by:
�9o
%
q
 
 e�W� ���� }
�9o lhs
# n '
q

o
n
q

o
%
q
The outside probability of the start symbol is 1.
We prove by induction over the depth of the parse
forest that the following relationship holds:

o
%
q

�(o
%
q

o
%
q

o
{
��q
It is easy to see that the assumption holds for
the root symbol
{
� :

o
{
�$q

s

�9o
{
�$q

o
{
� q

o
{
��q
The flow in a parse forest is computed by:

o
%
q
 
 e��  ���} }

o lhs
# n '
q

o
n
q

o lhs
# n '
q
Now, we insert the induction hypothesis:

o
%
q
 
 e��W  ���} }
�9o lhs
# n '
q

o lhs
# n '
q

o
n
q

o
{
�$q

o lhs
# n '
q
After a few transformations, we get the equation

o
%
q


o
%
q

o
{
�$q

 e��W� ���� }
�9o lhs
# n '
q

o
n
q

o
%
q
which is equivalent to

o
%
q

�9o
%
q

o
%
q

o
{
�mq
according to the definition of �9o
%
q . So, the induction hypothesis is generally true.
B Parse Forest Lexicalization
The function LEXICALIZE below takes an unlexicalized parse forest as argument and returns a
lexicalized parse forests, where each symbol is
uniquely labeled with a lexical head. Symbols are
split if they have more than one lexical head.
LEXICALIZE(v )
1 initialize v 1 as an empty parse forest
2 initialize array �o wϧi
y
��q����
3 for
%
in
y
�
4 do
%
1R� NEWT
#&�
�
n�� #&%9'�'
5 ΢o
%
q����
%
1&�
6 for n
in iz� in bottom-up order
7 do assume rhs
# n ' 
�
%
T
Q�%

QWVWVWVQ�%
! �
8 assume
%
 is the head of n
9 for
%
1T
%
1
VWVWV}%
1!
�
΢o
%
Tq
 VWVWV 
΢o
%
! q
10 do if
%

��y
�
11 then
"
� LEM
#8|
�
# n '�'
12 else
"
�h� � o
%
1 q
13 �
1 �pI
%
1T
VWVWV}%
1! P
14
%
 � lhs
# n '
15 r' � ADD(v 1
Q n Q�"�Q �
1 )
16 ΢o
%
 q����o
%
 q6j� lhs
# n
1
'
�
17 return v1
LEXICALIZE creates new terminal symbols by
calling the function NEWT. The new symbols are
linked to the original ones by means of �o� q . For
each rule in the old parse forest, the set of all
possible combinations of the lexicalized daughter symbols is generated. The function LEM
# n '
returns the lemma associated with lexical rule n
.
ADD(v
Q n Q�"�Q �
)
1 if �
%ڨ
΢o lhs
# n '
q s.t. �ڧ�o
%
q
 "
2 then
%
1 �
%
3 else
%
1R� NEWNT
#
v
'
4
|
�
#&%
1
'
�
|
�
#
lhs
# n '�'
5 ���o
%
1q��
"
6 n
1R� NEWRULE
#
v
Q�%
1
Q � '
7
|
�
# n
1
'
�
|
�
# n '
8 return n
1
For each combination of lexicalized daughter
symbols, a new rule is inserted by calling ADD.
ADD calls NEWNT to create new non-terminals
and NEWRULE to generate new rules. A nonterminal is only created if no symbol with the
same lexical head was linked to the original node.

