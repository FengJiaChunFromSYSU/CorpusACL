Refined Lexicon Models for Statistical Machine Translation using a
Maximum Entropy Approach
Ismael Garc�
ia Varea
Dpto. de Inform�
atica
Univ. de Castilla-La Mancha
Campus Universitario s/n
02071 Albacete, Spain
ivarea@info-ab.uclm.es
Franz J. Och and
Hermann Ney
Lehrstuhl f�
ur Inf. VI
RWTH Aachen
Ahornstr., 55
D-52056 Aachen, Germany
 
och|ney� @cs.rwth-aachen.de
Francisco Casacuberta
Dpto. de Sist. Inf. y Comp.
Inst. Tecn. de Inf. (UPV)
Avda. de Los Naranjos, s/n
46071 Valencia, Spain
fcn@iti.upv.es
Abstract
Typically, the lexicon models used in
statistical machine translation systems
do not include any kind of linguistic
or contextual information, which often
leads to problems in performing a correct word sense disambiguation. One
way to deal with this problem within
the statistical framework is to use maximum entropy methods. In this paper,
we present how to use this type of information within a statistical machine
translation system. We show that it is
possible to significantly decrease training and test corpus perplexity of the
translation models. In addition, we perform a rescoring of � -Best lists using our maximum entropy model and
thereby yield an improvement in translation quality. Experimental results are
presented on the so-called "Verbmobil
Task".
1 Introduction
Typically, the lexicon models used in statistical
machine translation systems are only single-word
based, that is one word in the source language corresponds to only one word in the target language.
Those lexicon models lack from context information that can be extracted from the same parallel corpus. This additional information could be:
� Simple context information: information of
the words surrounding the word pair;
� Syntactic information: part-of-speech information, syntactic constituent, sentence
mood;
� Semantic information: disambiguation information (e.g. from WordNet), current/previous speech or dialog act.
To include this additional information within the
statistical framework we use the maximum entropy approach. This approach has been applied
in natural language processing to a variety of
tasks. (Berger et al., 1996) applies this approach
to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in
translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those
proposed in (Brown et al., 1993). (Foster, 2000)
describes two methods for incorporating information about the relative position of bilingual word
pairs into a maximum entropy translation model.
Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al.,
1999; Peters and Klakow, 1999). A short review
of the maximum entropy approach is outlined in
Section 3.
2 Statistical Machine Translation
The goal of the translation process in statistical machine translation can be formulated as follows: A source language string ������ � �� � �
is to be translated into a target language string
� �  ��   . In the experiments reported in
this paper, the source language is German and the
target language is English. Every target string is
considered as a possible translation for the input.
If we assign a probability   �! ����#" to each pair
of strings  �%$ � �� " , then according to Bayes' decision rule, we have to choose the target string
that maximizes the product of the target language
model  � " and the string translation model
&����  � " .
Many existing systems for statistical machine
translation (Berger et al., 1994; Wang and Waibel,
1997; Tillmann et al., 1997; Nie�en et al., 1998)
make use of a special way of structuring the string
translation model like proposed by (Brown et al.,
1993): The correspondence between the words in
the source and the target string is described by
alignments that assign one target word position
to each source word position. The lexicon probability '(&�  " of a certain target word  to occur
in the target string is assumed to depend basically
only on the source word � aligned to it.
These alignment models are similar to the concept of Hidden Markov models (HMM) in speech
recognition. The alignment mapping is )10
2 �43!5 from source position ) to target position
2 �6375 . The alignment 3 �� may contain alignments 3758�@9 with the `empty' word A to account for source words that are not aligned to
any target word. In (statistical) alignment models
&� ��($ 3 �� %� " , the alignment 3 �� is introduced as
a hidden variable.
Typically, the search is performed using the socalled maximum approximation:
B � � C7DFEHGIC!P
QSRT
U
  � "WV!X`Yba
T
&� �� $ 3 ��  � "c
� C7DFEHGIC!P
Q RT
U
  � "WV GdC!P
Y a
T &� �� $ 3 ��  � "c
The search space consists of the set of all possible
target language strings %� and all possible alignments 3 �� .
The overall architecture of the statistical translation approach is depicted in Figure 1.
3 Maximum entropy modeling
The translation probability &� �� $ 3 �� %� " can be
rewritten as follows:
&� �� $ 3 ��e  �f" �
�
g
5ih �
pq&� 5 $ 3!5 �
5r �
� $ 3
5%r �
� $  �f"
�
�
g
5ih �Hs
 3!5 �
5r �
� $ 3
5%r �
� $  � "tV
&� 5 �
5%r �
� $ 3
5
� $  � "Su
Source Language Text
Transformation
Lexicon Model
Language Model
Global Search:
Target Language Text
over
Pr(f1
J
|e1
I
)
Pr( e1
I
)
Pr(f1
J
|e1
I
)
Pr( e1
I
)
e1
I
f1
J
maximize
Alignment Model
Transformation
Figure 1: Architecture of the translation approach
based on Bayes' decision rule.
Typically, the probability pq&� 5 �
5r �
� $ 3
5
� $ � " is
approximated by a lexicon model '(&� 5 
Ywv
" by
dropping the dependencies on �
5r �
� , 3
5%r �
� , and  � .
Obviously, this simplification is not true for a lot
of natural language phenomena. The straightforward approach to include more dependencies in
the lexicon model would be to add additional dependencies(e.g. 't&� 5 
YSv
$ 
YwvFx
T " ). This approach
would yield a significant data sparseness problem.
Here, the role of maximum entropy (ME) is to
build a stochastic model that efficiently takes a
larger context into account. In the following, we
will use 't&� y" to denote the probability that the
ME model assigns to � in the context y in order
to distinguish this model from the basic lexicon
model '(&�  " .
In the maximum entropy approach we describe
all properties that we feel are useful by so-called
feature functions ( y $ � " . For example, if we
want to model the existence or absence of a specific word  in the context of an English word 
which has the translation � we can express this
dependency using the following feature function:
 QfQw  y $ � " �
U
if � � �  and   y
9 otherwise
(1)
The ME principle suggests that the optimal
parametric form of a model '(&� y" taking into
account only the feature functions  $ �

$  $F is given by:
'(&� y" �


 y"(
P
sed
X
 h �f
g y $ � "Su
Here

 y" is a normalization factor. The resulting model has an exponential form with free
parameters
f
 $ �

$  $F . The parameter
values which maximize the likelihood for a given
training corpus can be computed with the socalled GIS algorithm (general iterative scaling)
or its improved version IIS (Pietra et al., 1997;
Berger et al., 1996).
It is important to notice that we will have to obtain one ME model for each target word observed
in the training data.
4 Contextual information and training
events
In order to train the ME model ' Q &� y" associated
to a target word  , we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample,
we need to know the word-to-word alignment between each sentence pair within the corpus. That
is obtained using the Viterbi alignment provided
by a translation model as described in (Brown et
al., 1993). Specifically, we use the Viterbi alignment that was produced by Model 5. We use the
program GIZA++ (Och and Ney, 2000b; Och and
Ney, 2000a), which is an extension of the training
program available in EGYPT (Al-Onaizan et al.,
1999).
Berger et al. (1996) use the words that surround a specific word pair   $ � " as contextual information. The authors propose as context the 3
words to the left and the 3 words to the right of
the target word. In this work we use the following
contextual information:
� Target context: As in (Berger et al., 1996) we
consider a window of 3 words to the left and
to the right of the target word considered.
� Source context: In addition, we consider a
window of 3 words to the left of the source
word � which is connected to  according to
the Viterbi alignment.
� Word classes: Instead of using a dependency
on the word identity we include also a dependency on word classes. By doing this, we
improve the generalization of the models and
include some semantic and syntactic information with. The word classes are computed
automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the
same semantic meaning in the same class.
A training event, for a specific target word  , is
composed by three items:
� The source word � aligned to  .
� The context in which the aligned pair   $ � "
appears.
� The number of occurrences of the event in
the training corpus.
Table 1 shows some examples of training events
for the target word "which".
5 Features
Once we have a set of training events for each target word we need to describe our feature functions. We do this by first specifying a large pool
of possible features and then by selecting a subset
of "good" features from this pool.
5.1 Features definition
All the features we consider form a triple
('ehgi $ label-1$ label-2) where:
� pos: is the position that label-2 has in a specific context.
� label-1: is the source word � of the aligned
word pair   $ � " or the word class of the
source word � (jk&� " ).
� label-2: is one word of the aligned word pair
  $ � " or the word class to which these words
belong (jk&� " $ml   " ).
Using this notation and given a context y :
y � n ro pp %n pp %nrq o � 5%ro pp � 5
Table 1: Some training events for the English word "which". The symbol " " is the placeholder of the
English word "which" in the English context. In the German part the placeholder (" ") corresponds
to the word aligned to "which", in the first example the German word "die", the word "das" in the
second and the word "was" in the third. The considered English and German contexts are separated by
the double bar "p".The last number in the rightmost position is the number of occurrences of the event
in the whole corpus.
Alig. word (� ) Context (y ) # of occur.
die bar there , I just already nette Bar , 2
das hotel best , is very centrally ein Hotel , 1
was now , one do we jetzt , 1
Table 2: Meaning of different feature categories where s represents a specific target word and t represents a specific source word.
Category  QSu  y $ � 5 " �

if and only if ...
1 � 5v� t
2 � 5 � t and sHw x y u
2 � 5v� t and sHw y u x
3 � 5v� t and sHw x x x y u
3 � 5v� t and sHw y u x x x
6 � 5v� t and z{w x |
v
7 � 5 � t and z{w x x x |
v
for the word pair  n $ � 5 " , we use the following
categories of features:
1. (9 $ � 5 $ )
2. (}

$ � 5 $   ) and   � nr~ �
3. (} $ � 5 $   ) and   n ro pp nrq o7
4. (}

$ j&� 5 " $ml    " ) and   �  nr~ �
5. (} $ j&� 5 " $ml   " ) and   n ro pp %npq og
6. (

$ � 5 $ �  ) and �  � � 5%r �
7. ( $ � 5 $ �  ) and �   � 5%ro pp � 5r � 
8. (

$ j&� 5 " $ jk&�  " ) and �  � � 5r �
9. ( $ j&� 5 " $ jk&�  " ) and �  � 5%ro p � 5r � 
Category 1 features depend only on the source
word � 5 and the target word %n . A ME model that
uses only those, predicts each source translation
� 5 with the probability ' Q &� 5 " determined by the
empirical data. This is exactly the standard lexicon probability '(&�  " employed in the translation model described in (Brown et al., 1993) and
in Section 2.
Category 2 describes features which depend in
addition on the word   one position to the left or
to the right of %n . The same explanation is valid
for category 3 but in this case   could appears in
any position of the context y . Categories 4 and
5 are the analogous categories to 2 and 3 using
word classes instead of words. In the categories
6, 7, 8 and 9 the source context is used instead of
the target context. Table 2 gives an overview of
the different feature categories.
Examples of specific features and their respective category are shown in Table 3.
Table 3: The 10 most important features and their
respective category and
f
values for the English
word "which".
Category Feature
f
1 (0,was,) 1.20787
1 (0,das,) 1.19333
5 (3,F35,E15) 1.17612
4 (1,F35,E15) 1.15916
3 (3,das,is) 1.12869
2 (1,das,is) 1.12596
1 (0,die,) 1.12596
5 (-3,was,@@) 1.12052
6 (-1,was,@@) 1.11511
9 (-3,F26,F18) 1.11242
5.2 Feature selection
The number of possible features that can be used
according to the German and English vocabularies and word classes is huge. In order to reduce the number of features we perform a threshold based feature selection, that is every feature
which occurs less than  times is not used. The
aim of the feature selection is two-fold. Firstly,
we obtain smaller models by using less features,
and secondly, we hope to avoid overfitting on the
training data.
In order to obtain the threshold  we compare
the test corpus perplexity for various thresholds.
The different threshold used in the experiments
range from 0 to 512. The threshold is used as a
cut-off for the number of occurrences that a specific feature must appear. So a cut-off of 0 means
that all features observed in the training data are
used. A cut-off of 32 means those features that
appear 32 times or more are considered to train
the maximum entropy models.
We select the English words that appear at least
150 times in the training sample which are in total
348 of the 4673 words contained in the English
vocabulary. Table 4 shows the different number
of features considered for the 348 English words
selected using different thresholds.
In choosing a reasonable threshold we have to
balance the number of features and observed perplexity.
Table 4: Number of features used according to
different cut-off threshold. In the second column
of the table are shown the number of features used
when only the English context is considered. The
third column correspond to English, German and
Word-Classes contexts.
# features used
 English English+German
0 846121 1581529
2 240053 500285
4 153225 330077
8 96983 210795
16 61329 131323
32 40441 80769
64 28147 49509
128 21469 31805
256 18511 22947
512 17193 19027
6 Experimental results
6.1 Training and test corpus
The "Verbmobil Task" is a speech translation task
in the domain of appointment scheduling, travel
planning, and hotel reservation. The task is difficult because it consists of spontaneous speech
and the syntactic structures of the sentences are
less restricted and highly variable. For the rescoring experiments we use the corpus described in
Table 5.
Table 5: Corpus characteristics for translation
task.
German English
Train Sentences 58 332
Words 519 523 549 921
Vocabulary 7 940 4 673
Test Sentences 147
Words 1 968 2 173
PP (trigr. LM) (40.3) 28.8
To train the maximum entropy models we used
the "Ristad ME Toolkit" described in (Ristad,
1997). We performed 100 iteration of the Improved Iterative Scaling algorithm (Pietra et al.,
1997) using the corpus described in Table 6,
Table 6: Corpus characteristics for perplexity
quality experiments.
German English
Train Sentences 50 000
Words 454 619 482 344
Vocabulary 7 456 4 420
Test Sentences 8073
Words 64 875 65 547
Vocabulary 2 579 1 666
which is a subset of the corpus shown in Table 5.
6.2 Training and test perplexities
In order to compute the training and test perplexities, we split the whole aligned training corpus
in two parts as shown in Table 6. The training
and test perplexities are shown in Table 7. As
expected, the perplexity reduction in the test corpus is lower than in the training corpus, but in
both cases better perplexities are obtained using
the ME models. The best value is obtained when
a threshold of 4 is used.
We expected to observe strong overfitting effects when a too small cut-off for features gets
used. Yet, for most words the best test corpus
perplexity is observed when we use all features
including those that occur only once.
Table 7: Training and Test perplexities using different contextual information and different
thresholds  . The reference perplexities obtained
with the basic translation model 5 are TrainPP =
10.38 and TestPP = 13.22.
English English+German
 TrainPP TestPP TrainPP TestPP
0 5.03 11.39 4.60 9.28
2 6.59 10.37 5.70 8.94
4 7.09 10.28 6.17 8.92
8 7.50 10.39 6.63 9.03
16 7.95 10.64 7.07 9.30
32 8.38 11.04 7.55 9.73
64 9.68 11.56 8.05 10.26
128 9.31 12.09 8.61 10.94
256 9.70 12.62 9.20 11.80
512 10.07 13.12 9.69 12.45
6.3 Translation results
In order to make use of the ME models in a statistical translation system we implemented a rescoring algorithm. This algorithm take as input the
standard lexicon model (not using maximum entropy) and the 348 models obtained with the ME
training. For an hypothesis sentence  � and a corresponding alignment 3 �� the algorithm modifies
the score &� ��($ 3 �� %� " according to the refined
maximum entropy lexicon model.
We carried out some preliminary experiments
with the � -best lists of hypotheses provided by
the translation system in order to make a rescoring of each i-th hypothesis and reorder the list according to the new score computed with the refined lexicon model. Unfortunately, our � -best
extraction algorithm is sub-optimal, i.e. not the
true best � translations are extracted. In addition,
so far we had to use a limit of only

9 translations
per sentence. Therefore, the results of the translation experiments are only preliminary.
For the evaluation of the translation quality
we use the automatically computable Word Error Rate (WER). The WER corresponds to the
edit distance between the produced translation
and one predefined reference translation. A shortcoming of the WER is the fact that it requires a
perfect word order. This is particularly a problem for the Verbmobil task, where the word order of the German-English sentence pair can be
quite different. As a result, the word order of
the automatically generated target sentence can
be different from that of the target sentence, but
nevertheless acceptable so that the WER measure
alone can be misleading. In order to overcome
this problem, we introduce as additional measure
the position-independent word error rate (PER).
This measure compares the words in the two sentences without taking the word order into account.
Depending on whether the translated sentence is
longer or shorter than the target translation, the
remaining words result in either insertion or deletion errors in addition to substitution errors. The
PER is guaranteed to be less than or equal to the
WER.
We use the top-10 list of hypothesis provided
by the translation system described in (Tillmann
and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the
new maximum entropy score. The translation results in terms of error rates are shown in Table 8.
We use Model 4 in order to perform the translation experiments because Model 4 typically gives
better translation results than Model 5.
We see that the translation quality improves
slightly with respect to the WER and PER. The
translation quality improvements so far are quite
small compared to the perplexity measure improvements. We attribute this to the fact that the
algorithm for computing the � -best lists is suboptimal.
Table 8: Preliminary translation results for the
Verbmobil Test-147 for different contextual information and different thresholds using the top-10
translations. The baseline translation results for
model 4 are WER=54.80 and PER=43.07.
English English+German
 WER PER WER PER
0 54.57 42.98 54.02 42.48
2 54.16 42.43 54.07 42.71
4 54.53 42.71 54.11 42.75
8 54.76 43.21 54.39 43.07
16 54.76 43.53 54.02 42.75
32 54.80 43.12 54.53 42.94
64 54.21 42.89 54.53 42.89
128 54.57 42.98 54.67 43.12
256 54.99 43.12 54.57 42.89
512 55.08 43.30 54.85 43.21
Table 9 shows some examples where the translation obtained with the rescoring procedure is
better than the best hypothesis provided by the
translation system.
7 Conclusions
We have developed refined lexicon models for
statistical machine translation by using maximum
entropy models. We have been able to obtain a
significant better test corpus perplexity and also a
slight improvement in translation quality. We believe that by performing a rescoring on translation
word graphs we will obtain a more significant improvement in translation quality.
For the future we plan to investigate more refined feature selection methods in order to make
the maximum entropy models smaller and better
generalizing. In addition, we want to investigate
more syntactic, semantic features and to include
features that go beyond sentence boundaries.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan Melamed,
David Purdy, Franz J. Och, Noah A. Smith,
and David Yarowsky. 1999. Statistical machine translation, final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-finalreport.ps.
A. L. Berger, P. F. Brown, S. A. Della Pietra, et al.
1994. The candide system for machine translation.
In Proc. , ARPA Workshop on Human Language
Technology, pages 157�162.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Computational Linguistics, 22(1):39�72, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,
19(2):263�311.
George Foster. 2000. Incorporating position information into a maximum entropy/minimum divergence
translation model. In Proc. of CoNNL-2000 and
LLL-2000, pages 37�52, Lisbon, Portugal.
Sven Martin, Christoph Hamacher, J�
org Liermann,
Frank Wessel, and Hermann Ney. 1999. Assessment of smoothing methods and complex stochastic language modeling. In IEEE International Conference on Acoustics, Speech and Signal Processing, volume I, pages 1939�1942, Budapest, Hungary, September.
Sonja Nie�en, Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1998. A DP-based search
algorithm for statistical machine translation. In
COLING-ACL '98: 36th Annual Meeting of the Association for Computational Linguistics and 17th
Int. Conf. on Computational Linguistics, pages
960�967, Montreal, Canada, August.
Franz J. Och and Hermann Ney. 2000a. Giza++:
Training of statistical translation models.
http://www-i6.Informatik.RWTHAachen.DE/~och/software/GIZA++.html.
Franz J. Och and Hermann Ney. 2000b. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational
Linguistics, pages 440�447, Hongkong, China, October.
Table 9: Four examples showing the translation obtained with the Model 4 and the ME model for a
given German source sentence.
SRC: Danach wollten wir eigentlich noch Abendessen gehen.
M4: We actually concluding dinner together.
ME: Afterwards we wanted to go to dinner.
SRC: Bei mir oder bei Ihnen?
M4: For me or for you?
ME: At your or my place?
SRC: Das w�
are genau das richtige.
M4: That is exactly it spirit.
ME: That is the right thing.
SRC: Ja, das sieht bei mir eigentlich im Januar ziemlich gut aus.
M4: Yes, that does not suit me in January looks pretty good.
ME: Yes, that looks pretty good for me actually in January.
Franz J. Och. 1999. An efficient method for determining bilingual word classes. In EACL '99: Ninth
Conf. of the Europ. Chapter of the Association for
Computational Linguistics, pages 71�76, Bergen,
Norway, June.
K.A. Papineni, S. Roukos, and R.T. Ward. 1996.
Feature-based language understanding. In ESCA,
Eurospeech, pages 1435�1438, Rhodes, Greece.
K.A. Papineni, S. Roukos, and R.T. Ward. 1998.
Maximum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf. on
Acoustics, Speech, and Signal Processing, pages
189�192.
Jochen Peters and Dietrich Klakow. 1999. Compact
maximum entropy language models. In Proceedings of the IEEE Workshop on Automatic Speech
Recognition and Understanding, Keystone, CO,
December.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features in random fields.
IEEE Trans. on Pattern Analysis and Machine Inteligence, 19(4):380�393, July.
Eric S. Ristad. 1997. Maximum entropy modelling
toolkit. Technical report, Princeton Univesity.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187�228.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical machine translation. In 8th International Conference on Computational Linguistics (CoLing 2000),
pages 850�856, Saarbr�
ucken, Germany, July.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in
statistical translation. In Proc. 35th Annual Conf.
of the Association for Computational Linguistics,
pages 289�296, Madrid, Spain, July.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical translation. In Proc. 35th Annual
Conf. of the Association for Computational Linguistics, pages 366�372, Madrid, Spain, July.

