Grammars for Local and Long Dependencies.
Alexander Dikovsky
Universit�
e de Nantes, IRIN, 2, rue de la Houssini`
ere
BP 92208 F 44322 Nantes cedex 3 France
Alexandre.Dikovsky@irin.univ-nantes.fr
Abstract
Polarized dependency (PD-) grammars
are proposed as a means of efficient
treatment of discontinuous constructions. PD-grammars describe two kinds
of dependencies : local, explicitly derived by the rules, and long, implicitly
specified by negative and positive valencies of words. If in a PD-grammar
the number of non-saturated valencies
in derived structures is bounded by a
constant, then it is weakly equivalent
to a cf-grammar and has a
 �������� time parsing algorithm. It happens that
such bounded PD-grammars are strong
enough to express such phenomena as
unbounded raising, extraction and extraposition.
1 Introduction
Syntactic theories based on the concept of dependency have a long tradition. Tesni`
ere (Tesni`
ere,
1959) was the first who systematically described
the sentence structure in terms of binary relations
between words (dependencies), which form a dependency tree (D-tree for short). D-tree itself
does not presume a linear order on words. However, any its surface realization projects some linear order relation (called also precedence). Some
properties of surface syntactic structure can be expressed only in terms of both dependency (or its
transitive closure called dominance) and precedence. One of such properties, projectivity, requires that any word occurring between a word
� and a word  dependent on � be dominated by
� In first dependency grammars (Gaifman, 1961)
and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective
dependency grammars (Lombardo and Lesmo,
1996) the projectivity is implied by definition. In
some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence
of this property, D-trees are in a sense equivalent to phrase structures with head selection 1.
It is for this reason that D-trees determined by
grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some
other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead
to polynomial time algorithms (cf.
 ��� � � -time
algorithm for link grammars in (Sleator and Temperly, 1993)). Meanwhile, the projectivity is not
the norm in natural languages. For example, in
most European languages there are such regular non-projective constructions as WH- or relative clause extraction, topicalization, comparative constructions, and some constructions specific to a language, e.g. French pronominal clitics or left dislocation. In terms of phrase structure, non-projectivity corresponds to discontinuity. In this form it is in the center of discussions till 70-ies. There are various dependency based approaches to this problem. In
the framework of Meaning-Text Theory (Mel'
cuk
and Pertsov, 1987), dependencies between (some1
See (Dikovsky and Modina, 2000) for more details.
times non adjacent) words are determined in
terms of their local neighborhood, which leads
to non-tractable parsing (the NP-hardness argument of (Neuhaus and Br�
oker, 1997) applies to
them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo
and Lesmo, 1998; Br�
oker, 1998)) impose on nonprojective D-trees some constraints weaker than
projectivity (cf. meta-projectivity (Nasr, 1995) or
pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing
algorithm. Still another approach is developed
in the context of intuitionistic resource-dependent
logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for
Lambek calculus). In this context, non-projective
D-trees are determined with the use of hypothetical reasoning and of structural rules such as commutativity and associativity (see e.g. (Moortgat,
1990)).
In this paper, we put forward a novel approach to handling discontinuity in terms of dependency structures. We propose a notion of a
polarized dependency (PD-) grammar combining
several ideas from cf-tree grammars, dependency
grammars and resource-dependent logics. As
most dependency grammars, the PD-grammars
are analyzing. They reduce continuous groups
to their types using local (context-free) reduction
rules and simultaneously assign partial dependency structures to reduced groups. The valencies
(positive for governors and negative for subordinates) are used to specify discontinuous (long) dependencies lacking in partial dependency structures. The mechanism of establishing long dependencies is orthogonal to reduction and is implemented by a universal and simple rule of valencies saturation. A simplified version of PDgrammars adapted for the theoretical analysis is
introduced and explored in (Dikovsky, 2001). In
this paper, we describe a notion of PD-grammar
more adapted for practical tasks.
2 Dependency structures
We fix finite alphabets  of terminals (words), 
of nonterminals (syntactic types or classes), and
 of dependency names.
Definition 1. Let  � ! �#" be a string. A
set $%'&�)(10 22035476 of trees (called components
of $ ) which cover exactly 80 have no nodes in
common, and whose arcs are labeled by names
in
 is a dependency (D-) structure on  if one
component @9 of $ is selected as its head 2. We
use the notation A%CB � $ �  $ is a terminal Dstructure if  is a string of terminals. When $ has
only one component, it is a dependency (D-) tree
on  
For example, the D-structure in Fig. 1 has
two components. D �FEHG  � is the root of the non
projective head component, the other component
IQPSRT�U�FV1G7E@�8� is a unit tree.
WYXa`cbed bgfhSi3X2p�qsrutwv fxySp WYX d bgfyx�b#p dd qsrUt

 ySF

xyS

 hS
Fig. 1.
In distinction to (Dikovsky, 2001), the nonterminals (and even dependency names) can be
structured. We follow (Mel'
cuk and Pertsov,
1987) and distinguish syntactical  �S� and morphological edgfih features of a nonterminal  
The alphabets being finite, the features unification
is a means of compacting a grammar.
The D-structures we will use will be polarized
in the sense that some words will have valencies
specifying long dependencies which must enter
or go from them. A valency is an expression of
one of the forms jQkml P , jonpl P (a positive valency), or qrkl P , qrnpl P (a negative valency),
P being a dependency name. For example, the
intuitive sense of a positive valency jonpl P of
a node
� is that a long dependency
P might go
from
� somewhere on the right. All nonterminals
will be signed: we presume that  is decomposed
into two classes : of positive (ts"vu ) and negative
( sxw u ) nonterminals respectively. D-structures
with valencies, DV-structures, are defined so that
valencies saturation would imply connectivity.
Definition 2. A terminal
� is polarized if a finite
list of pairwise different valencies 3 D ���8� (its
valency list) is assigned to it.
� is positive, if
D ���8� does not contain negative valencies, A
D-tree with polarized nodes is positive if its root
2
We visualize yz underlining it or its root, when there are
some other components.
3
In the original definition of (Dikovsky, 2001), valencies
may repeat in
v f2bHp{ but this seems to be a natural constraint.
is positive, otherwise it is negative.
A D-structure $ on a string  of polarized
symbols is a DV-structure on 80 if the following
conditions are satisfied :
(v1) if a terminal node
� of $ is negative, then
D ���8� contains exactly one negative valency,
(v2) if a dependency of $ enters a node
� 0 then
� is positive,
(v3) the non-head components of $ (if any)
are all negative.
The polarity of a DV-structure is that of its head.
y|h  b~}8 x y ihib y i3b    i  X 

 
|SQhSXih |   
5   h a
y i#iX 
hSXih |
f d Xihp
5 
f y a{ }8 p WYXa`cb

y iFiX  
hSXih |
`oUWu v  v   u  Y  
  WYX|  f d Xaih5p   2    y|h  b
x   } i  x  Xai3
�@��hSXih |   F


 ySF y� X  � bH�   
WYX`�bgf�hiX|p�WYX v fc y pWYX v f�� �  y|h  bHp
x�  
y� X  � bH�   F

 ySF
y ihib y ib    i  X 
  
 h |
`�b WYXa`cb

 h a
  
d�  |8 v  v U  u  Y  
Fig. 2.
In Fig. 2 4, both words in $�(�� have no valencies,
all nonterminals in $�(( and $v( � are positive (we
label only negative nonterminals), $�(�� is positive
because its head component is a positive unit Dtree, $�a( and $��� are negative because their roots
are negative.
Valencies are saturated by long dependencies.
Definition 3. Let $ be a terminal DV-structure.
A triplet ��%�d � (0 � �10 P hT0 where
� (0 � � are
nodes of $ and
P   0 is a long dependency
4
For the reasons of space, in our examples we
are not accurate with morphological features. E.g.,
in the place of GrV(gov:upon) we should rather have
GrV(gov:upon)
q inf
t .
with the name
P 0 directed from
� ( to
� � (notation:
� ( �
qq�h � � ), if there are valencies � ( 
D ��� ( � 0�S�T�D ��� � � such that :
(v4) either
� ( d � � (
� ( precedes
� � ), � ( %
jonpl P , and ��r%iqckl P , or
(v5)
� ��d � (01�(�%�jokl P , and ��r%�qcnel P .
We will say that �( saturates �� by long dependency � .
The set of valencies in $ is totally ordered by
the order of nodes and the orders in their valency
lists: �(�d��S� if
(o1) either �(CD)� ��� ( � 0�S��CD)� ��� � � and
� (�d � �0
(o2) or �(0��o�Dg� ���8� and �(�d��S0 in Dg� ���8� 
Let $�( be the structure resulting from $ by
adding the long dependency � and replacing
Dg� ��� ( � by D)�  ��� ( � %eDg� ��� ( �� &�H(6 and Dg� ��� � �
by Dg�  ��� � � %�Dg� ��� � ��� &��6  We will say that $v(
is a saturation of $ by � and denote it by $?�$v( 
Among all possible saturations of $ we will select
the following particular one :
Let �(��D)� ��� ( � be the first non saturated
positive valency in $u0 and ��� Dg� ��� � � be
the closest corresponding 5 non saturated negative valency in $  Then the long dependency
�%�� � ( �
q�q�h � �a� saturating �� by �( is first
available (FA) in $  The resulting saturation of $
by � is first available or FA-saturation (notation:
$�r�v��$v( ).
We transform the relations ��0� �v� into partial
orders closing them by transitivity.
x x   y  
   
 

X
x x   y  
   
 
 
X
X
x x   y  
�  
 
Fig. 3.
Suppose that in Fig. 3, both occurrences of
E
in $�� and the first occurrence of
E in $�( have
D �FE˧ %�&5qrn!l P 650 and both occurrences of �
in $ � and the second occurrence of � in $ ( have
D � � � %e&jok�l P 6  Then $�o� �v� $v(s� �v� $� 
5
Corresponding means :
(c1)
b  q�b  and �|� � X if �7� �u�� X{ and
(c2)
b  q�b  and �  � �� X if �  � �u� X�
In (Dikovsky, 2001), we prove that
� If $ is a terminal DV-structure and $ҿ�$v(0
then either $�( has a cycle, or it is a DV-structure
(Lemma 1).
As it follows from Definition 3, each saturation of a terminal DV-structure $ has the same set
of nodes and a strictly narrower set of valencies.
Therefore, any terminal DV-structure has maximal saturations with respect to the order relations
�T0u� �v�  Very importantly, there is a single maximal FA-saturation of $ denoted �p� ( � $ �  E.g.,
in Fig. 3, ��� ( � $� � %A$�� is a D-tree.
In order to keep track of those valencies which
are not yet saturated we use the following notion
of integral valency.
Definition 4. Let $ be a terminal DV-structure.
The integral valency �
�v�
$ of $ is the list
�
4�٩4s� s� u D��� 
s� u ���8� ordered by the order of valencies in $  If ��� ( � $ � is a d-tree, we say that
this D-tree saturates $ and call $ saturable.
By this definition, �
�v�
��� ( � $ � % �
�v�
$ 
Saturability is easily expressed in terms of
integral valency (Lemma 2 in (Dikovsky, 2001)) :
Let $ be a terminal DV-structure. Then :
� ��� ( � $ � is a D-tree iff it is cycle-free and
�
�v�
$�%i�0
� $ has at most one saturating D-tree.
The semantics of PD-grammars will be defined
in terms of composition of DV-structures which
generalizes strings substitution.
Definition 5. Let $ ( % &� ( 0 2203@�6 be a DVstructure, � be a nonterminal node of one of its
components, and $�%�&�5�( 0 2203@�9 0 2203@��6 be a
DV-structure of the same polarity as � and with
the head component 5�9  Then the result of the
composition of $ � into $ ( in � is the DV-structure
$v(��� � $�a�0 in which $�� is substituted for ��0 the
root of @�9 inherits all dependencies of � in $v(0
and the head component is that of $v( (changed
respectively if touched on by composition)6.
It is easy to see that DV-structure $ in Fig. 4
can be derived by the following series of compo6
This composition generalizes the substitution used in
TAGs (Joshi et al., 1975) (
 needs not be a leaf) and is not
like the adjunction.
sitions of the DV-structures in Fig. 2:
$�(��r%A$v((�� �8� H�#��� � H� ���a� 0 IQP  �8�1�� �� PS� �0
$���r%A$�a(��FRQP �#� � w �1G �� � 0 � ��0B � B �gEH� 0
IoP  �8� $�(���0
$ (� %A$ ( � �IoP  �U��7� P@�� Bo�0 IoP D � f��S �� f EH� 0
IoP D � � �l G �7� �8��1P 訲 � �0
$�%A$�(��5�IoP  �FRQP �� � w � $���S0
Q�����q G �7� �8� $�(�|�0
and �%i�p� ( � $ � 
y|h  b!}8 x y ihib y i3b    i  X  } i  x  Xi3



 

y|h  b }8 x y ihib y i3b    i  X  } i  x  Xi3



 
| �  � � 
hSXih |   
DV-structure  (
f �   p ��� f�5�   p hSXih |   F )
D-tree y ϶�   f p
Fig. 4.
The DV-structures composition has natural
properties:
� The result of a composition into a DVstructure $ is a DV-structure of the same polarity
as $ (Lemma 3 in (Dikovsky, 2001)).
� If �
�v�
$ ( % �
�v�
$ � 0 then �
�v�
$ � �� � ��� ( � $ ( � �
% �
�v�
$���� � ��� ( � $� � � for any terminal $v(03$�
(Lemma 4 in (Dikovsky, 2001)).
3 Polarized dependency grammars
Polarized dependency grammars determine DVstructures in the bottom-up manner in the course
of reduction of phrases to their types, just as the
categorial grammars do. Each reduction step is
accompanied by DV-structures composition and
by subsequent FA-saturation. The yield of a successful reduction is a D-tree. In this paper, we
describe a superclass of grammars in (Dikovsky,
2001) which are more realistic from the point of
view of real applications and have the same parsing complexity.
Definition 6. A PD-grammar is a system
I %
� �0�0  03�g0�03n � 0 where �0�0  are as described above, �g�A�s"vu is a set of axioms (which
are positive nonterminals), �� ���k is a
ternary relation of lexical interpretation, k being
y|h  b }8 x y ihSi3b y ib    i  X  } i  x  Xai3



 

 h |
y i#Fi3X 
hSXih |
prepos-obj
 y dir-inf-obj
WYX`�bgf�hiXap
WYX v fc y p
GrV(gov:upon)
GrNn
Nn
GrNn
Cl/obj-upon
(Adj,wh)
f d Xihp 
WYX|  f2y|h  bp 
ClWh
+L:prepos-obj
-R:prepos-obj
X  X 
X �
X  
Fig. 5.
the set of lists of pairwise different valencies, and
n is a set of reduction rules. For simplicity,
we start with the strict reduction rules (the only
rules in (Dikovsky, 2001)) of the form $�����0
where �� and $ is a DV-structure over  of
the same polarity as A (below we will extend the
strict rules by side effects). In the special case,
where the DV-structures in the rules are D-trees,
the PD-grammar is local7.
Intuitively, we can think of � as of the combined information available after the phase of
morphological analysis (i.e. dictionary information and tags). So
� B�03��0�H� � �� means that a
type � and a valency list �H� can be a priori assigned to the word B 
Semantics. 1. Let
P % � B�03��0�� � �� and $�
be the unit DV-structure B with D � B � %��H�  Then
P is a reduction of the structure $�� to its type �
(notation $ʡ  � � ) and �H� is the integral valency
of this reduction denoted by ��% �
�
$� 
2. Let
P % � $�� � � be a reduction rule
with � nonterminals' occurrences ��(0 2203�c� in $u0
��h���0 and $v(� ��  ��(0 220@$� ���Q�� be some
reductions. Then t% � ( 22H� P5� is a reduction of
the structure $ʴ%���� ( � $���( � $v(0 2203�� � $��� �
to its type � (notation $ �  ��e� ).  ( 0 220H�
as well as  itself are subreductions of   The
integral valency of $� via  is �
�
$�� %
�
�v�
$���( � $v(0 2203�� � $��% �
�v�
$�  A D-tree  is
determined by
I if there is a reduction  ��
7
Local PD-grammars are strongly equivalent to dependency tree grammars of (Dikovsky and Modina, 2000) which
are generating and not analyzing as here.
�Y0�� ��  The DT-language determined by
I is the set  �I� of all D-trees it determines.
k �Iȧ %�&B �  � � �Iȧ 6 is the language
determined by
I ! �FR  I� denotes the class of
languages determined by PD-grammars.
By way of illustration, let us consider the
PD-grammar
I � with the lexical interpretation �
containing triplets:
� Bo�0 IoP  �U��7� P@� 0��� � 0 � f E@� 0 IoP D � f�� � 0��� �
��P � � 0 IoP D � � �Ul G �7� �u� 0��jokgl�� P ���  q���#�S� � 0
��G �� � 0 �FRQP �#� � w0��qrnl�� P ���  q���#�5� � 0
� B �gEH� 0 � ��50B  0�� � � 0 � H�#��� � H� ���a� 0  � 0��� � 0
���� �� PS� 0 IoP  � 0��� � 0 and the following reduction
rules whose left parts are shown in Fig. 2:
P (�% � $v((�� IQP  �8� 0
P �r% � $v( � � T����#�vq G �� �8� 0
P � % � $�a(�� IQP  �8��G �� �8� w � 0
P � % � $ (�� � T� �7� 
Then the D-tree  in Fig. 4 is reducible in
I � to
T� � and its reduction is depicted in Fig. 5.
As we show in (Dikovsky, 2001), the weak
generative capacity of PD-grammars is stronger
than that of cf-grammars. For example, the PDgrammar
I ( :
W  

"
  y
 

"
 
x
|� X|

�u�  X|
generates a non-cf language &B ���8�# B ���8� %
E 4 � 4  � 4 0 �%$ �6  D-tree $ � in Fig. 3 is determined by
I ( on B �'&5�  Its reduction combined
with the diagram of local and long dependencies
is presented in Fig. 6.
y
 
 
 
 


x

x

(
(
Fig. 6.
The local PD-grammars are weakly equivalent
to cf-grammars, so they are weaker than general
PD-grammars. Meanwhile, what is really important concerning the dependency grammars, is
their strong generative capacity, i.e. the D-trees
they derive. From this point of view, the grammars like
I ( above are too strong. Let us remark
that in the reduction in Fig. 6, the first saturation
becomes possible only after all positive valencies
emerge. This means that the integral valency of
subreductions increases with
�  This seems to be
never the case in natural languages, where next
valencies arise only after the preceding ones are
saturated. This is why we restrict ourself to the
class of PD-grammars which have such a property.
Definition 7. Let
I be a PD-grammar. For a
reduction  of a terminal structure, its defect is
defined as ) �  � %�f E &  �
�10
$�� 2 H� is a subreduction of 6  I has bounded (unbounded) defect if there is some (there is no) constant
V which
bounds the defect of all its reductions. The minimal constant
V having this property (if any) is the
defect of
I (denoted ) �I� ).
There is a certain technical problem concerning
PD-grammars. Even if in a reduction to an axiom
all valencies are saturated, this does not guarantee that a D-tree is derived: the graph may have
cycles. In (Dikovsky, 2001) we give a sufficient
condition for a PD-grammar of never producing
cycles while FA-saturation. We call the grammars
satisfying this condition lc- (locally cycle-) free.
For the space reasons, we don't cite its definition, the more so that the linguistic PD-grammars
should certainly be lc-free. In (Dikovsky, 2001)
we prove the following theorem.
Theorem 1. For any lc-free PD-grammar
I of
bounded defect there is an equivalent cf-grammar.
Together with this we show an example of a
DT-language which cannot be determined by local PD-grammars. This means that not all structures determined in terms of long dependencies
can be determined without them.
4 Side effect rules and parsing
An important consequence of Theorem 1 is
that lc-free bounded defect PD-grammars have a
 ������ parsing algorithm. In fact, it is the classical Earley algorithm in charter form (the charters being DV-structures). To apply this algorithm in practice, we should analyze the asymptotic factor which depends on the size of the
grammar. The idea of theorem 1 is that the integral valency being bounded, it can be compiled into types. This means that a reduction
rule $ � � should be substituted by rules
$���( � �T(1�Dv(�0 2203�� � ��)�Dg��2� � ��D�a� with
types keeping all possible integral valencies not
causing cycles. Theoretically, this might blow
up � � s43 " ( u times the size of a grammar with defect
V 0 � valencies and the maximal length �
of left parts of rules. So theoretically, the constant factor in the
 ������ time bound is great. In
practice, it shouldn't be as awful, because in linguistic grammars
V will certainly equal 5S0 one
rule will mostly treat one valency (i.e. ��%65 )
and the majority of rules will be local. Practically, the effect may be that some local rules will
have variants propagating upwards a certain valency: $�7 � 7�@�2�� ��@�  The actual problem lies elsewhere. Let us analyze the illustration
grammar
I � and the reduction in Fig. 5. This
reduction is successful due to the fact that the
negative valency qrnl�� P �#��  q���#� is assigned to
the preposition
G �� � and the corresponding positive valency jokgl�� P ���  q��S�#� is assigned to the
verb
P 訲 �  What might serve the formal basis for
these assignments? Let us start with
P � �  This
verb has the strong government over prepositions
� � 0 G �7� �  In the clause in Fig. 4, the group of
the preposition is moved, which is of course a
sufficient condition for assigning the positive valency to the verb. But this condition is not available in the dictionary, nor even through morphological analysis (
P 訲 � may occur at a certain distance from the end of the clause). So it can only
be derived in the course of reduction, but strict
PD-grammars have no rules assigning valencies.
Theoretically, there is no problem: we should
just introduce into the dictionary both variants of
the verb description � with the local dependency
� P �#��  q���#� to the right and with the positive valency jQkgl�� P �#�7�  q���#� to the left. Practically,
this "solution" is inacceptable because such a lexical ambiguity will lead to a brute force search.
The same argument shows that we shouldn't assign the negative valency qcnl�� P �#��  q���� to
G �� � in the dictionary, but rather "calculate" it in
the reduction. If we compare the clause in Fig. 4
with the clauses what theories we may rely upon;
what kind of theories we may rely upon; the dependency theories of what kind we may rely upon
etc., we see that we can assign a qrn valency to
wh-words in the dictionary and then raise negative valencies till the FA-saturation. The problem
is that in the strict PD-grammars there are no rules
of valency raising. For these reasons we extend
the reduction rules by side effects sufficient for
the calculations of both kinds.
Definition 8. We introduce two kinds of side
effects: valency raising
� � ( � �98� �@ � � � and
valency assignment
��98�BA � � 0 �0�(�0�� being
valency names and
8 an integer. A rule of the
form
$ � � ( � �98� �@ � � � � �
with � nonterminals ��(0 2203�c� in $ and
5DC 8 CE� is valency raising if :
(r1) �(0�� are of the same polarity,
(r2) a local dependency
P enters �c� in $ ,
(r3) for positive �(0��S0i$w� � is a strict
reduction rule,
(r4) if � ( 0� � are negative, then � � 03�� �sw u 0
and replacing �c� by any positive nonterminal we
obtain a DV-structure 8. A rule of the form
$ ��98�FA � � � �
with � nonterminals ��(0 2203�c� in $ and
5DC 8 CE� is valency assigning if :
(a1) for a positive �0'$ � � is a strict
8
So this occurrence of
HG in  contradicts to the point
(v2) of definition 2.
reduction rule,
(a2) if � is negative and �r� is the root of $u0
then �c�Y s"vu and �e� sw u 0
(a3) if � is negative and �c� is not the root
of $u0 then � !tsxw u 0�� � !ts"vu is a non
head component of $ 9 and replacing �c� by any
negative nonterminal we obtain a DV-structure.
Semantics. We change the reduction semantics
as follows.
� For a raising rule $ � �H(� �98#� �@ �� � � ��0
the result of the reduction is the DV-structure
$���%���� ( �PI�� ��03$���( � $v(0 2207�r� �RQ� �(�03$g� � 0 220
�� � $��� �� 0 where
QH� ��03$�� � is the DV-structure
resulting from $�� by deleting � from D ��P �S� �|� $�� �� 0
and
I�� �03$�� � is the DV-structure resulting from $��
by adding � to D ��P �S� �|� $�� �� 
� For a valency assignment rule
$ ��98� A � � � ��0 the result of the reduction is the DV-structure $�s%A��� ( � $���( � $v(0 220
�c� �SI�� �03$� � 0 2205�c� � $�� � 
A PD-grammar with side effect rules is a
PDSE-grammar.
This definition is correct in the sense that the
result of a reduction with side effects is always a
DV-structure. We can prove
Theorem 2. For any lc-free PDSE-grammar
I of
bounded defect there is an equivalent cf-grammar.
Moreover, the bounded defect PDSEgrammars are also parsed in time
 ��� � � 
In fact, we can drop negative �( in raising
rules (it is unique) and indicate the type of
P �S� �|� $ � in both side effect rules, because the
composition we use makes this information
local. Now, we can revise the grammar
I �
above, e.g. excluding the dictionary assignment
��G �� � 0 �FRQP �#� � w0��qrnl�� P ���  q���#�5� � 0 and using
in its place several valency raising rules such as:
f d Xih5p f y a{ }8 p WYX`�b fUTpWV WYX|  f2y|h  bHp 

y i#iX 

hSXih |
where )�% f'X`Yxfba1fxWYX`�bHppdcfehgbcfihp
q r
esgtc -R:prepos-obj
p�
5 Conclusion
The main ideas underlying our approach to discontinuity are the following:
9
So this occurrence of
uG in  contradicts to the point
(v3) of definition 2.
� Continuous (local, even if non projective)
dependencies are treated in terms of trees composition (which reminds TAGs). E.g., the French
pronominal clitics can be treated in this way.
� Discontinuous (long) dependencies are captured in terms of FA-saturation of valencies in
the course of bottom-up reduction of dependency
groups to their types. As compared with the
SLASH of GPSG or the regular expression lifting
control in non projective dependency grammars,
these means turn out to be more efficient under
the conjecture of bounded defect. This conjecture seems to be true for natural languages (the
contrary would mean the possibility of unlimited
extraction from extracted groups).
� The valency raising and assignment rules offer a way of deriving a proper valency saturation
without unwarranted increase of lexical ambiguity.
A theoretical analysis and experiments in English syntax description show that the proposed
grammars may serve for practical tasks and can
be implemented by an efficient parser.
6 Acknowledgments
I would like to express my heartfelt gratitude to
N. Pertsov for fruitful discussions of this paper.
The idea of valency raising has emerged from our
joint work over a project of a PD-grammar for a
fragment of English.
References
Y. Bar-Hillel, H. Gaifman, and E. Shamir. 1960. On
categorial and phrase structure grammars. Bull.
Res. Council Israel, 9F:1�16.
N. Br�
oker. 1998. Separating surface order and syntactic relations in a dependency grammar. In Proc.
COLING-ACL, pages 174�180, Montreal.
A.Ja. Dikovsky and L.S. Modina. 2000. Dependencies on the other side of the Curtain. Traitement
Automatique des Langues (TAL), 41(1):79�111.
A. Dikovsky. 2001. Polarized non-projective dependency grammars. In Ph. De Groote and G. Morrill,
editors, Logical Aspects of Computational Linguistics, number 2099 in LNAI. Springer Verlag. To be
published.
H. Gaifman. 1961. Dependency systems and phrase
structure systems. Report p-2315, RAND Corp.
Santa Monica (CA). Published in: Information and
Control, 1965, v. 8, n v 3, pp. 304-337.
R.A. Hudson. 1984. Word Grammar. Basil Blackwell, Oxford-New York.
A.K. Joshi, L.S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journ. of Comput. and Syst.
Sci., 10(v 1):136�163.
S. Kahane, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity : A polynomially parsable
non-projective dependency grammar. In Proc.
COLING-ACL, pages 646�652, Montreal.
J. Lambek. 1958. The mathematics of sentence structure. American Mathematical Monthly, pages 154�
170.
A. Lecomte. 1992. Proof nets and dependencies. In
Proc. of COLING-92, pages 394�401, Nantes.
V. Lombardo and L. Lesmo. 1996. An earley-type
recognizer for dependency grammar. In Proc. 16th
COLING, pages 723�728.
V. Lombardo and L. Lesmo. 1998. Formal aspects
and parsing issues of dependency theory. In Proc.
COLING-ACL, pages 787�793, Montreal.
I. Mel'
cuk and N.V. Pertsov. 1987. Surface Syntax of
English. A Formal Model Within the Meaning-Text
Framework. John Benjamins Publishing Company,
Amsterdam/Philadelphia.
M. Moortgat. 1990. La grammaire cat�
egorielle
g�
en�
eralis�
ee : le calcul de lambek-gentzen. In Ph.
Miller and Th. Torris, editors, Structure of languages and its mathematical aspects, pages 127�
182. Hermes, Paris.
A. Nasr. 1995. A formalism and a parser for lexicalized dependency grammars. In Proc. Int. Workshop
on Parsing Technology, pages 186�195, Prague.
P. Neuhaus and N. Br�
oker. 1997. The Complexity
of Recognition of Linguistically Adequate Dependency Grammars. In Proc. of 35th ACL Annual
Meeting and 8th Conf. of the ECACL, pages 337�
343.
Jane J. Robinson. 1970. Dependency structures and
transformational rules. Language, 46(v 2):259�
285.
D. D. Sleator and D. Temperly. 1993. Parsing English
with a Link Grammar. In Proc. IWPT'93, pages
277�291.
L. Tesni`
ere. 1959. �
El�
ements de syntaxe structurale.
Librairie C. Klincksieck, Paris.

